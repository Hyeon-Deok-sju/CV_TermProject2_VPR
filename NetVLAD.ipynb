{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "438a1b57-d64d-424c-a73f-6d0a2fecaa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x249de623150>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "from torchvision. models import vgg16\n",
    "\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e6538f67-d4a1-43c1-b402-d7cb289c379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base 경로\n",
    "os.chdir(\"D:/2021/2학기 수업/CV/VPR/NetVLAD_hhd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d5319c61-f845-4c0c-99f7-4bea8fc9cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\2021\\2학기 수업\\CV\\VPR\\NetVLAD_hhd\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "93729d8d-6d3c-4cb5-ab15-e1f6dfcfb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "18bc3ddc-f477-4e3e-be5d-8f3983731b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_clusters = 12, dim = 128, alpha = 100.0, normalize_input = True):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.normalize_input = normalize_input\n",
    "        self.conv = nn.Conv2d(dim, num_clusters, 1, bias = False) # 논문에서는 bias: True, --> False 는 version 2\n",
    "        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))\n",
    "        self._init_params()\n",
    "        \n",
    "    def _init_params(self):\n",
    "        # w_k = 2 * alpha * c_k\n",
    "        self.conv.weight = nn.Parameter(\n",
    "            (2.0 * self.alpha * self.centroids).unsqueeze(-1).unsqueeze(-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # N = 3: 3장의 이미지 (img, pos, neg) ; C = 512 (VGG 16 기준)\n",
    "        N, C = x.shape[:2]\n",
    "        \n",
    "        if self.normalize_input:\n",
    "            x = F.normalize(x, p = 2, dim = 1)\n",
    "        \n",
    "        # soft-assignment\n",
    "        soft_assign = self.conv(x).view(N, self.num_clusters, -1) # (3, 16, (W x H))\n",
    "        soft_assign = F.softmax(soft_assign, dim = 1)\n",
    "        \n",
    "        x_flatten = x.view(N, C, -1) # (3, 512, (W x H))\n",
    "        \n",
    "        vlad = torch.zeros([N, self.num_clusters, C], dtype = x.dtype, layout = x.layout, device = x.device) # (3 x 16 x 512) \n",
    "        \n",
    "        # x_flatten.unsqueeze(0).permute(1, 0, 2, 3): (3, 1, 512, (W x H))\n",
    "        # centroids: 16 x 512 --> 16개의 centroid, 512 차원\n",
    "        # centroids[0, :]: 1 x 512 --> 0 번째 centroid의 좌표\n",
    "        # centroids[0, :].expand(x_flatten.size(-1), -1, -1): ((W x H), 1, 512)        \n",
    "        for C in range(self.num_clusters):\n",
    "            residual = x_flatten.unsqueeze(0).permute(1, 0, 2, 3) - \\\n",
    "                self.centroids[C:C + 1, :].expand(x_flatten.size(-1), -1, -1).permute(1, 2, 0).unsqueeze(0)\n",
    "            residual *= soft_assign[:, C:C+1, :].unsqueeze(2)\n",
    "            vlad[:, C:C+1, :] = residual.sum(dim = -1)\n",
    "            \n",
    "        vlad = F.normalize(vlad, p = 2, dim = 2)\n",
    "        vlad = vlad.view(x.size(0), -1)\n",
    "        vlad = F.normalize(vlad, p = 2, dim = 1)\n",
    "        \n",
    "        return vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "602271ec-d212-4fe8-834a-14903b3859ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]],\n",
      "\n",
      "        [[1, 2, 3]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3]])\n",
    "print(a.shape)\n",
    "a = a.expand(10, -1, -1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3d604f1d-adba-4427-a413-7f552cd5da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = vgg16(pretrained = True)\n",
    "# vgg 16 layer 중 conv5-3 까지의 layer만 사용. (512개의 3 x 3 x 512 커널 필터 & ReLU는 사용 안 한 상태)\n",
    "layers = list(encoder.features.children())[:-2]\n",
    "#print(layers[-1])\n",
    "for l in layers[:-5]:\n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "model = nn.Module()\n",
    "\n",
    "encoder = nn.Sequential(*layers)\n",
    "model.add_module('encoder', encoder)\n",
    "\n",
    "dim = list(encoder.parameters())[-1].shape[0]\n",
    "\n",
    "#print(dim)\n",
    "#print(list(encoder.parameters())[-3].shape)\n",
    "\n",
    "net_vlad = NetVLAD(num_clusters = 16, dim = dim)\n",
    "model.add_module('pool', net_vlad)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f7548bda-d2df-4ae8-a9c3-e62878632af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = torch.load('./pittsburgh_checkpoint.pth.tar')\n",
    "model.load_state_dict(load_model['state_dict'])\n",
    "#print(load_model['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9567890b-33af-414f-9556-05c150b2be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from random import choice\n",
    "from os.path import join, exists\n",
    "from collections import namedtuple\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4e2e33cb-2922-451e-a45e-8812050ce261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dbStruct(path):\n",
    "    mat = loadmat(path)\n",
    "    \n",
    "    matStruct = mat['dbStruct'][0]\n",
    "    \n",
    "    dataset = 'dataset'\n",
    "    \n",
    "    whichSet = 'VPR'\n",
    "    \n",
    "    dbImage = matStruct[0]\n",
    "    locDb = matStruct[1]\n",
    "    \n",
    "    qImage = matStruct[2]\n",
    "    locQ = matStruct[3]\n",
    "    \n",
    "    numDb = matStruct[4].item()\n",
    "    numQ = matStruct[5].item()\n",
    "    \n",
    "    posDistThr = matStruct[6].item()\n",
    "    posDistSqThr = matStruct[7].item()\n",
    "    \n",
    "    return dbStruct(whichSet, dataset, dbImage, locDb, qImage, locQ, numDb, numQ, posDistThr, posDistSqThr)\n",
    "\n",
    "dbStruct = namedtuple('dbStruct', ['whichSet', 'dataset', 'dbImage', 'locDb', 'qImage', 'locQ', 'numDb',\n",
    "                                   'numQ', 'posDistThr', 'posDistSqThr'])\n",
    "\n",
    "class BerlinDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, condition = 'train'):\n",
    "        self.dbStruct = parse_dbStruct('berlin.mat')\n",
    "        self.input_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                                 std = [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        self.condition = condition\n",
    "        \n",
    "        if self.condition == 'train':\n",
    "            self.images = [join(os.getcwd(), dblm.replace(' ','')) for dblm in self.dbStruct.dbImage]\n",
    "        elif self.condition == 'test':\n",
    "            self.images = [join(os.getcwd(), qlm.replace(' ','')) for qlm in self.dbStruct.qImage]\n",
    "        else:\n",
    "            self.images = [join(os.getcwd(), dblm.replace(' ','')) for dblm in self.dbStruct.dbImage]\n",
    "            \n",
    "        self.positives = None\n",
    "        self.distances = None\n",
    "        \n",
    "        self.getPositives()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.condition == 'train':\n",
    "            img = Image.open(self.images[idx])\n",
    "            img = self.input_transform(img)\n",
    "            \n",
    "            pos_list = self.positives[idx].tolist()\n",
    "            pos_list.remove(idx)\n",
    "            pos_idx = self.positives[idx][np.random.randint(0, len(self.positives[idx]))]\n",
    "            pos_img = Image.open(self.images[pos_idx])\n",
    "            pos_img = self.input_transform(pos_img)\n",
    "            \n",
    "            pos_list = pos_list + [idx]\n",
    "            neg_idx = choice([i for i in range(len(self.images)) if i not in pos_list])\n",
    "            neg_img = Image.open(self.images[neg_idx])\n",
    "            neg_img = self.input_transform(neg_img)\n",
    "            img = torch.stack([img, pos_img, neg_img], dim=0)\n",
    "            label = torch.Tensor([0, 0, 1])\n",
    "            \n",
    "            return img, label\n",
    "        \n",
    "        \n",
    "        elif self.condition == 'test':\n",
    "            img = Image.open(self.images[idx])\n",
    "            img = self.input_transform(img)\n",
    "            \n",
    "            return img\n",
    "        \n",
    "        else:\n",
    "            img = Image.open(self.images[idx])\n",
    "            img = self.input_transform(img)\n",
    "            \n",
    "            return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def getPositives(self):\n",
    "        if self.condition == 'train':\n",
    "            knn = NearestNeighbors(n_jobs = 1)\n",
    "            knn.fit(self.dbStruct.locDb)\n",
    "            \n",
    "            self.distances, self.positives = knn.radius_neighbors(self.dbStruct.locDb, radius=self.dbStruct.posDistThr)\n",
    "        else:\n",
    "            knn = NearestNeighbors(n_jobs = 1)\n",
    "            knn.fit(self.dbStruct.locDb)\n",
    "            \n",
    "            self.distances, self.positives = knn.radius_neighbors(self.dbStruct.locQ, radius=self.dbStruct.posDistThr)\n",
    "        \n",
    "        return self.positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d8fb967e-52ff-4816-ac9d-04d43cd1ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BerlinDataset(condition = 'train')\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1, shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c6de51ed-dd94-4275-97cf-bd3b0c501734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "647f3341-4a6f-4d46-a015-16028ac6914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, batch_idx : 20, triplet_loss : 0.13946509928930373\n",
      "epoch : 0, batch_idx : 40, triplet_loss : 0.1362961792364353\n",
      "epoch : 0, batch_idx : 60, triplet_loss : 0.15165122512911186\n",
      "epoch : 0, batch_idx : 80, triplet_loss : 0.14034086171491647\n",
      "epoch : 0, batch_idx : 100, triplet_loss : 0.13942886402111243\n",
      "epoch : 0, batch_idx : 120, triplet_loss : 0.13726824324978285\n",
      "epoch : 0, batch_idx : 140, triplet_loss : 0.1331700530458004\n",
      "epoch : 0, batch_idx : 160, triplet_loss : 0.1355095076264802\n",
      "epoch : 0, batch_idx : 180, triplet_loss : 0.13900799612972617\n",
      "epoch : 0, batch_idx : 200, triplet_loss : 0.14055891327597014\n",
      "epoch : 0, batch_idx : 220, triplet_loss : 0.1403128102893743\n",
      "epoch : 0, batch_idx : 240, triplet_loss : 0.14231352044339002\n",
      "epoch : 0, batch_idx : 260, triplet_loss : 0.14059329535312579\n",
      "epoch : 0, batch_idx : 280, triplet_loss : 0.13697510639543636\n",
      "epoch : 0, batch_idx : 300, triplet_loss : 0.13731027679189892\n",
      "epoch : 1, batch_idx : 20, triplet_loss : 0.13362771005772833\n",
      "epoch : 1, batch_idx : 40, triplet_loss : 0.13103161764816498\n",
      "epoch : 1, batch_idx : 60, triplet_loss : 0.13468660386403403\n",
      "epoch : 1, batch_idx : 80, triplet_loss : 0.13396615106848223\n",
      "epoch : 1, batch_idx : 100, triplet_loss : 0.1345819733229028\n",
      "epoch : 1, batch_idx : 120, triplet_loss : 0.13679091053447504\n",
      "epoch : 1, batch_idx : 140, triplet_loss : 0.13725727914453864\n",
      "epoch : 1, batch_idx : 160, triplet_loss : 0.13763884883177907\n",
      "epoch : 1, batch_idx : 180, triplet_loss : 0.13927583971408883\n",
      "epoch : 1, batch_idx : 200, triplet_loss : 0.14062912892369392\n",
      "epoch : 1, batch_idx : 220, triplet_loss : 0.142122491943502\n",
      "epoch : 1, batch_idx : 240, triplet_loss : 0.1420227267720678\n",
      "epoch : 1, batch_idx : 260, triplet_loss : 0.14109409124954886\n",
      "epoch : 1, batch_idx : 280, triplet_loss : 0.13954337164133537\n",
      "epoch : 1, batch_idx : 300, triplet_loss : 0.1404461083373403\n",
      "epoch : 2, batch_idx : 20, triplet_loss : 0.14078629090348818\n",
      "epoch : 2, batch_idx : 40, triplet_loss : 0.14176883603068924\n",
      "epoch : 2, batch_idx : 60, triplet_loss : 0.1440192575205566\n",
      "epoch : 2, batch_idx : 80, triplet_loss : 0.14432168175034193\n",
      "epoch : 2, batch_idx : 100, triplet_loss : 0.14452267935900695\n",
      "epoch : 2, batch_idx : 120, triplet_loss : 0.14448207545184963\n",
      "epoch : 2, batch_idx : 140, triplet_loss : 0.14292137629033996\n",
      "epoch : 2, batch_idx : 160, triplet_loss : 0.14298375810204955\n",
      "epoch : 2, batch_idx : 180, triplet_loss : 0.14321220819970734\n",
      "epoch : 2, batch_idx : 200, triplet_loss : 0.14360017253061255\n",
      "epoch : 2, batch_idx : 220, triplet_loss : 0.143753899702335\n",
      "epoch : 2, batch_idx : 240, triplet_loss : 0.14389115160436158\n",
      "epoch : 2, batch_idx : 260, triplet_loss : 0.14516514083800086\n",
      "epoch : 2, batch_idx : 280, triplet_loss : 0.14452682030607503\n",
      "epoch : 2, batch_idx : 300, triplet_loss : 0.14410094867096523\n",
      "epoch : 3, batch_idx : 20, triplet_loss : 0.14402180681099896\n",
      "epoch : 3, batch_idx : 40, triplet_loss : 0.14394227291778533\n",
      "epoch : 3, batch_idx : 60, triplet_loss : 0.14342699111518215\n",
      "epoch : 3, batch_idx : 80, triplet_loss : 0.14358247684010778\n",
      "epoch : 3, batch_idx : 100, triplet_loss : 0.1432939660972054\n",
      "epoch : 3, batch_idx : 120, triplet_loss : 0.1430231700051201\n",
      "epoch : 3, batch_idx : 140, triplet_loss : 0.1433959094771388\n",
      "epoch : 3, batch_idx : 160, triplet_loss : 0.14444467147047263\n",
      "epoch : 3, batch_idx : 180, triplet_loss : 0.14421037388824504\n",
      "epoch : 3, batch_idx : 200, triplet_loss : 0.14410509551082487\n",
      "epoch : 3, batch_idx : 220, triplet_loss : 0.14419742016985002\n",
      "epoch : 3, batch_idx : 240, triplet_loss : 0.1436144219347711\n",
      "epoch : 3, batch_idx : 260, triplet_loss : 0.1434674776303996\n",
      "epoch : 3, batch_idx : 280, triplet_loss : 0.1437501189948493\n",
      "epoch : 3, batch_idx : 300, triplet_loss : 0.1440449941570684\n",
      "epoch : 4, batch_idx : 20, triplet_loss : 0.14322777866103636\n",
      "epoch : 4, batch_idx : 40, triplet_loss : 0.14322319482047463\n",
      "epoch : 4, batch_idx : 60, triplet_loss : 0.14377930219551788\n",
      "epoch : 4, batch_idx : 80, triplet_loss : 0.1437654391811602\n",
      "epoch : 4, batch_idx : 100, triplet_loss : 0.14318675870129355\n",
      "epoch : 4, batch_idx : 120, triplet_loss : 0.14367367512260387\n",
      "epoch : 4, batch_idx : 140, triplet_loss : 0.14295665250988457\n",
      "epoch : 4, batch_idx : 160, triplet_loss : 0.14262799179377317\n",
      "epoch : 4, batch_idx : 180, triplet_loss : 0.14219861227186836\n",
      "epoch : 4, batch_idx : 200, triplet_loss : 0.14273846022626188\n",
      "epoch : 4, batch_idx : 220, triplet_loss : 0.1427643601363304\n",
      "epoch : 4, batch_idx : 240, triplet_loss : 0.14312280749350925\n",
      "epoch : 4, batch_idx : 260, triplet_loss : 0.14267656157970743\n",
      "epoch : 4, batch_idx : 280, triplet_loss : 0.1427968183196669\n",
      "epoch : 4, batch_idx : 300, triplet_loss : 0.14257969060767975\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "global_batch_size = 8\n",
    "lr = 0.00001\n",
    "momentum = 0.9\n",
    "weightDecay = 0.001\n",
    "losses = AvgMeter()\n",
    "best_loss = 100.0\n",
    "margin = 0.1\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin = margin ** 0.5, p = 2, reduction = 'sum').cuda()\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr = lr, \n",
    "                            momentum = momentum, weight_decay = weightDecay)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (train_image, train_label) in enumerate(train_loader):\n",
    "        output_train = model.encoder(train_image.squeeze().cuda())\n",
    "        \n",
    "        #if batch_idx == 0:\n",
    "            #print(train_image.squeeze().shape) # 3 x 3 x 480 x 640: img 3장(img, pos, neg), dim(channel), H, W\n",
    "            #print(output_train.shape) # 3 x 512 x 30 x 40: H, W 의 크기 1/2^4 으로 줄어듬. vgg16 참고\n",
    "            \n",
    "        output_train = model.pool(output_train)\n",
    "        \n",
    "        #if batch_idx == 0:\n",
    "            #print(output_train.shape)\n",
    "        triplet_loss = criterion(output_train[0].reshape(1, -1),\n",
    "                                 output_train[1].reshape(1, -1),\n",
    "                                 output_train[2].reshape(1, -1))\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            optimizer.zero_grad()\n",
    "            #print(triplet_loss.item())\n",
    "        \n",
    "        triplet_loss.backward(retain_graph = True)\n",
    "        losses.update(triplet_loss.item())\n",
    "        \n",
    "        if (batch_idx + 1) % global_batch_size == 0:\n",
    "            for name, p in model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad /= global_batch_size\n",
    "                    #if batch_idx == 7:\n",
    "                        #print(p.shape)\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "        if batch_idx % 20 == 0 and batch_idx != 0:\n",
    "            print('epoch : {}, batch_idx : {}, triplet_loss : {}'.format(epoch, batch_idx, losses.avg))\n",
    "    \n",
    "    if best_loss > losses.avg:\n",
    "        best_path = 'out_model/best_model.pt'\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "    \n",
    "    model_save_name = 'out_model/model_{:02d}.pt'.format(epoch)\n",
    "    torch.save(model.state_dict(), model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "27c0bfe6-04d2-422c-9c16-a544acf7415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "314it [00:09, 33.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cluster_dataset = BerlinDataset(condition = 'cluster')\n",
    "cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size = 1, shuffle = False, num_workers = 0)\n",
    "\n",
    "train_feature_list = list()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, train_img in tqdm(enumerate(cluster_loader)):\n",
    "        output_train = model.encoder(train_img.cuda())\n",
    "        output_train = model.pool(output_train)\n",
    "        train_feature_list.append(output_train.squeeze().detach().cpu().numpy())\n",
    "\n",
    "train_feature_list = np.array(train_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4e82dcdd-6053-48ca-9b28-7b017e33f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BerlinDataset(condition = 'test')\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef379780-2be0-4b30-a8d6-3323170bb76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:09, 28.08it/s]\n"
     ]
    }
   ],
   "source": [
    "test_feature_list = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, test_img in tqdm(enumerate(test_loader)):\n",
    "        output_test = model.encoder(test_img.cuda())\n",
    "        output_test = model.pool(output_test)\n",
    "        test_feature_list.append(output_test.squeeze().detach().cpu().numpy())\n",
    "        \n",
    "test_feature_list = np.array(test_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3ba802dc-f7de-40a7-b10d-ac15aaa9f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "n_values = [1, 5, 10, 20]\n",
    "faiss_index = faiss.IndexFlatL2(train_feature_list.shape[1])\n",
    "faiss_index.add(train_feature_list)\n",
    "_, predictions = faiss_index.search(test_feature_list, max(n_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a3ca1ea4-cc91-410b-97bf-6d5095965b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"./submit.json\"\n",
    "\n",
    "data = {}\n",
    "data['Query'] = list()\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    data_t = [(\"id\",i),(\"positive\",predictions[i].tolist())]\n",
    "    data_t = dict(data_t)\n",
    "    data['Query'].append(data_t)\n",
    "\n",
    "with open(file_path, 'w') as outfile:\n",
    "    json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da242b-4d2f-4039-9b56-a879e7f668b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
